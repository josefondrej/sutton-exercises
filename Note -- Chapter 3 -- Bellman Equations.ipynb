{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It truly is intuitively clear that \n",
    "\n",
    "\n",
    "$$v_{*}(s) = max_{a \\in \\mathcal{A}(s)} q_{\\pi_{*}}(s,a)$$\n",
    "\n",
    "Let's assume that there really exists an optimal policy $\\pi_{*}$. This policy has by definition the property that no matter what action** $a(s)$ we select in each state (and step*), we can never get better expected total reward than if we used action(s) $a_{*}(s)$ given by the optimal policy. \n",
    "\n",
    "So if we select some arbitrary action in the first step and then select actions according to the optimal policy, we can not get better expected total reward than if we used the optimal actions the whole time. Obviously if we select the first action according to the optimal policy, than we get precisely the optimal expected total reward, which is exactly what the equation says. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* The whole MDP thing is invariant to how many steps we took in the past. It only cares about the current state and action. We could consider policies that depend on state and the number of steps we took. But because of the Markov property, there must be an optimal policy (suppose there is one), that depends on the state only. -- **TODO write this down formally**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\** We can view action as probability distribution on actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
