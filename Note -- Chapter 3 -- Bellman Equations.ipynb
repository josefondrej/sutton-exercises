{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the Optimal Policy Exists It Must Satisfy the Bellman Optimality Eqs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It truly is intuitively clear that \n",
    "\n",
    "\n",
    "$$v_{*}(s) = max_{a \\in \\mathcal{A}(s)} q_{\\pi_{*}}(s,a)$$\n",
    "\n",
    "Let's assume that there really exists an optimal policy $\\pi_{*}$. This policy has by definition the property that no matter what action\\* $a(s)$ we select in each state (and step\\*\\*), we can never get better expected total reward than if we used action(s) $a_{*}(s)$ given by the optimal policy. \n",
    "\n",
    "So if we select some arbitrary action in the first step and then select actions according to the optimal policy, we can not get better expected total reward than if we used the optimal actions the whole time. Obviously if we select the first action according to the optimal policy, than we get precisely the optimal expected total reward, which is exactly what the equation says. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* We can view action as probability distribution on actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*\\* The whole MDP thing is invariant to how many steps we took in the past. It only cares about the current state and action. We could consider policies that depend on state and the number of steps we took. But because of the Markov property, there must be an optimal policy (suppose there is one), that depends on the state only. If we have optimal policy $\\pi_{t}(s)$ then $\\pi_{1}(s)$ is also an optimal policy that does not depend on the number of steps we took."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of Existence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not clear at all, that the optimal policy should exist. The proof can be found for example in this video: https://www.youtube.com/watch?v=ybEyXc4hNuk and it goes something like this. \n",
    "\n",
    "Denote $N = dim \\mathcal{S}$ the size of the set of states. \n",
    "\n",
    "Let $V$ be a the vector space of state-value functions $v = \\big ( v(s_{1}), \\dots, v(s_{N}) \\big )$. If we use the max norm on this space, it can be shown that it is complete (not clear -- **TODO**). \n",
    "\n",
    "Lets define $L$ as operator on $V$ s.t. \n",
    "\n",
    "$$Lv = max_{\\pi} r_{\\pi} + \\gamma P_{\\pi} v$$\n",
    "\n",
    "where $r_{\\pi} = \\big ( r_{\\pi}(s_{1}), \\dots, r_{\\pi}(s_{N}) \\big)$ is the expected reward in one step for policy $\\pi$ and $P_{\\pi}$ is a $N \\times N$ matrix where $P_{\\pi}(i,j) = Proba(S_{t+1} = j \\mid S_{t} = i)$ under the policy $\\pi$. \n",
    "\n",
    "It can be shown that $L$ is a contraction and the result (existence and uniqueness of solution to Bellman optimality eqs.) follows from the Banach fixed point theorem. \n",
    "\n",
    "WLOG lets $0 \\leq Lv(s) - Lu(s).$\n",
    "\n",
    "Denote $a^{*}_{s} \\in argmax_{a} E[r \\mid s,a] + \\gamma \\sum_{j} p(j \\mid s,a) v(j)$. Then we have that \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "Lv(s) - Lu(s) & \\leq E[r \\mid s,a^{*}_{s}] + \\gamma \\sum_{j} p(j \\mid s,a^{*}_{s}) v(j) \\\\ & -  E[r \\mid s,a^{*}_{s}] + \\gamma \\sum_{j} p(j \\mid s,a^{*}_{s}) u(j) \\\\ \n",
    "& \\leq \\gamma || u - v ||_{max}\n",
    "\\end{align}\n",
    "\n",
    "This implies $ ||Lv(s) - Lu||_{max} \\leq \\gamma ||u-v||_{max} $, so $L$ is a contraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
