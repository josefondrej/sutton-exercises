{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3 --  Markov Decision Processes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sutton:**\n",
    "\n",
    "$$ \\big ( \\mathcal{S}, \\mathcal{A}, \\mathcal{R}, p(s^{\\prime}, r \\mid s, a), \\gamma \\big ) $$\n",
    "\n",
    "all $\\mathcal{S, A, R}$ finite.\n",
    "\n",
    "**Wikipedia:**\n",
    "\n",
    "$$ \\big ( \\mathcal{S}, \\mathcal{A}, p(s^{\\prime} \\mid s, a), r(s^{\\prime}, s, a), \\gamma \\big ) $$\n",
    "\n",
    "$\\mathcal{S, A}$ are finite, $r$ is expected reward received after transitioning from $s$ to $s^{\\prime}$ due to action $a$. \n",
    "\n",
    "Suttons version is more general in the sense that distribution on rewards can't be reconstructed from it's expected value. Wikipedia is more general in the sense that it allows for $\\mathcal{R}$ to be infinite. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**States, Actions, Rewards**\n",
    "\n",
    "$S_{t} \\in \\mathcal{S}$ ... *state* of the environment\n",
    "\n",
    "$A_{t} \\in \\mathcal{A(s)} \\subset \\mathcal{A}$ ... *action*\n",
    "\n",
    "$R_{t} \\in \\mathcal{R}$ ... *reward* \n",
    "\n",
    "**Definition of MDP** \n",
    "\n",
    "$p(s^{\\prime}, r  \\mid  s, a) \\doteq P(S_{t+1} = s^{\\prime}, R_{t+1} = r \\mid S_{t} = s, A_{t} = a)$\n",
    "\n",
    "**Return**\n",
    "\n",
    "$G_{t} \\doteq R_{t+1} + R_{t+2} + \\dots + R_{T}$ ... *episodic tasks* \n",
    "\n",
    "$G_{t} \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} \\dots $ ... *continuing tasks* with *discount rate* $\\gamma$\n",
    "\n",
    "$G_{t} \\doteq \\sum_{k = t+1}^{T} \\gamma^{k-t-1}R_{k}$\n",
    "\n",
    "**Policy**\n",
    "\n",
    "$\\pi(s \\mid a) : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ ... probability of selecting action $a$ at state $s$\n",
    "\n",
    "**Value of state under policy**\n",
    "\n",
    "$v_{\\pi}(s) = E_{\\pi}[G_t \\mid S_t = s]$ ... *state-value function* for policy $\\pi$\n",
    "\n",
    "$q_{\\pi}(s, a) = E_{\\pi}[G_t \\mid S_{t} = s, A_{t} = a]$ ... *action-value function* for policy $\\pi$\n",
    "\n",
    "**Bellman equations** \n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{s^{\\prime}, r} p(s^{\\prime}, r \\mid s, a) (r + \\gamma v_{\\pi}(s^{\\prime})) ~~ \\forall s \\in \\mathcal{S}$$\n",
    "\n",
    "$$q_{\\pi}(s, a) = \\sum_{s^{\\prime}, r} p(s^{\\prime}, r \\mid s, a) \\big (r + \\gamma \\sum_{a^{\\prime}} \\pi(a^{\\prime} \\mid s^{\\prime}) q_{\\pi}(s^{\\prime}, a^{\\prime})\\big)$$\n",
    "\n",
    "Note: the second equation is solution to Exercise 3.13"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
